# -*- coding: utf-8 -*-
"""190601D_feature_engineering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FJY0pVvGbn4AW1UZfiAMy-zY0lWe_hDE

# ML Lab 01 - Feature Engineering

### Import neccessary Libararies
"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.decomposition import PCA
import numpy as np
import seaborn as sns
import statsmodels.api as sm
from statsmodels.formula.api import ols

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

from xgboost import XGBClassifier
from sklearn.metrics import mean_squared_error
from sklearn import svm

from imblearn.over_sampling import SMOTE
from sklearn.datasets import make_classification
from collections import Counter
from imblearn.over_sampling import RandomOverSampler

"""## Step 01 : Open the dataset and analyse"""

traindata_ori = pd.read_csv('train.csv')
traindata_ori.head()

validdata_ori = pd.read_csv('valid.csv')
validdata_ori.head()

"""### Check for datatypes"""

with pd.option_context('display.max_rows', None, 'display.max_columns', None):
    print(traindata_ori.dtypes)

# All the features are of type float64

# label_1          int64
# label_2        float64
# label_3          int64
# label_4          int64

"""### Check for missing values"""

with pd.option_context('display.max_rows', None, 'display.max_columns', None):
    print(traindata_ori.isnull().sum())

# No missing values in features
# label_1          0
# label_2        480
# label_3          0
# label_4          0

"""## Step 02 : Prepare the dataframes for each label separately"""

Speaker_ID_train_ori = traindata_ori.drop(columns=['label_2','label_3','label_4'])
Speaker_age_train_ori = traindata_ori.drop(columns=['label_1','label_3','label_4'])
Speaker_gender_train_ori = traindata_ori.drop(columns=['label_1','label_2','label_4'])
Speaker_accent_train_ori = traindata_ori.drop(columns=['label_1','label_2','label_3'])

Speaker_ID_valid_ori = validdata_ori.drop(columns=['label_2','label_3','label_4'])
Speaker_age_valid_ori = validdata_ori.drop(columns=['label_1','label_3','label_4'])
Speaker_gender_valid_ori = validdata_ori.drop(columns=['label_1','label_2','label_4'])
Speaker_accent_valid_ori = validdata_ori.drop(columns=['label_1','label_2','label_3'])

"""## Step 03 : Analysis

### Label 01 - Speaker ID
"""

Speaker_ID_train_ori.head()

value_counts = Speaker_ID_train_ori['label_1'].value_counts()
plt.figure(figsize=(10, 6))
value_counts.plot(kind='bar')
plt.title('Value Counts of Category')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

"""#### Co-relation analysis"""

correlation_matrix = Speaker_ID_train_ori.corr()
correlation_matrix

"""##### Filter Features with High Correlation"""

threshold = 0.6

mask = np.abs(correlation_matrix) > threshold
correlated_features = set()

for i in range(len(mask.columns)):
    for j in range(i):
        if mask.iloc[i, j]:
            colname= correlation_matrix.columns[i] if correlation_matrix.iloc[i,-1] <= correlation_matrix.iloc[j,-1] else correlation_matrix.columns[j]
            correlated_features.add(colname)

Speaker_ID_train_dropped =  Speaker_ID_train_ori.drop(columns=correlated_features)
Speaker_ID_valid = Speaker_ID_valid_ori.drop(columns=correlated_features)

Speaker_ID_train_dropped.shape

Speaker_ID_valid.head()

Speaker_ID_train_dropped.head()

"""#### PCA"""

PCA_analysis_df = Speaker_ID_train_dropped.drop(columns=['label_1'])

PCA_analysis_df_valid = Speaker_ID_valid.drop(columns=['label_1'])

# Standardizing the data (important for PCA)
scaler = StandardScaler()
scaled_data = scaler.fit_transform(PCA_analysis_df)

scaled_valid_data = scaler.transform(PCA_analysis_df_valid)

pca = PCA(n_components=0.96,svd_solver='full')
principal_components = pca.fit_transform(scaled_data)

valid_principal_components = pca.transform(scaled_valid_data)

principal_df = pd.DataFrame(data=principal_components, columns=[f'Principal Component {i}' for i in range(principal_components.shape[1])])
valid_principal_df = pd.DataFrame(data = valid_principal_components,columns=[f'Principal Component {i}' for i in range(valid_principal_components.shape[1])])

principal_df.shape

"""#### Fit a Random Forest classifier"""

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(principal_df, Speaker_ID_train_ori['label_1'])

valid_predictions = rf_model.predict(valid_principal_df)

accuracy = accuracy_score(Speaker_ID_valid_ori['label_1'], valid_predictions)
print(f"Validation Accuracy: {accuracy:.2f}")

"""### Label 02 - Speaker Age

First we have to handle the missing values. Since there's enough data to train I will get rid of the training data with missing values of label_2
"""

Speaker_age_train_cleaned = Speaker_age_train_ori.dropna()
Speaker_age_valid_cleaned = Speaker_age_valid_ori.dropna()

Speaker_age_valid_cleaned.shape

"""#### Correlation analysis"""

correlation_matrix = Speaker_age_train_cleaned.corr()
correlation_matrix

"""##### Filter features with high correlation"""

threshold = 0.5

mask = np.abs(correlation_matrix) > threshold
correlated_features = set()

for i in range(len(mask.columns)):
    for j in range(i):
        if mask.iloc[i, j]:
            colname= correlation_matrix.columns[i] if correlation_matrix.iloc[i,-1] <= correlation_matrix.iloc[j,-1] else correlation_matrix.columns[j]
            correlated_features.add(colname)

Speaker_age_train_dropped =  Speaker_age_train_cleaned.drop(columns=correlated_features)
Speaker_age_valid_dropped = Speaker_age_valid_cleaned.drop(columns=correlated_features)

Speaker_age_valid_dropped.shape

"""#### PCA"""

PCA_analysis_df = Speaker_age_train_dropped.drop(columns=['label_2'])

PCA_analysis_df_valid = Speaker_age_valid_dropped.drop(columns=['label_2'])

# Standardizing the data (important for PCA)
scaler = StandardScaler()
scaled_data = scaler.fit_transform(PCA_analysis_df)

scaled_valid_data = scaler.transform(PCA_analysis_df_valid)

pca = PCA(n_components=0.97,svd_solver='full')
principal_components = pca.fit_transform(scaled_data)

valid_principal_components = pca.transform(scaled_valid_data)

principal_df = pd.DataFrame(data=principal_components, columns=[f'Principal Component {i}' for i in range(principal_components.shape[1])])
valid_principal_df = pd.DataFrame(data = valid_principal_components,columns=[f'Principal Component {i}' for i in range(valid_principal_components.shape[1])])

principal_df.shape

"""#### Fit a Random Forest Classifier"""

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(principal_df, Speaker_age_train_cleaned['label_2'])

valid_predictions = rf_model.predict(valid_principal_df)

accuracy = accuracy_score(Speaker_age_valid_cleaned['label_2'], valid_predictions)
print(f"Validation Accuracy: {accuracy:.2f}")

"""#### Fit a SVM Classifier"""

clf = svm.SVC()

clf.fit(principal_df, Speaker_age_train_cleaned['label_2'])
y_pred = clf.predict(valid_principal_df)

accuracy = accuracy_score(Speaker_age_valid_cleaned['label_2'], y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

"""### Label 03 - Speaker Gender

First let's analyse the distribution of different labels
"""

value_counts = Speaker_gender_train_ori['label_3'].value_counts()
plt.figure(figsize=(10, 6))
value_counts.plot(kind='bar')
plt.title('Value Counts of Category')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

value_counts

Speaker_gender_train_ori_X = Speaker_gender_train_ori.drop(columns=['label_3'])
Speaker_gender_train_ori_y = Speaker_gender_train_ori['label_3']

"""#### Handle the class imbalance using Random Oversampling"""

ros = RandomOverSampler(random_state=42)

X_resampled, y_resampled = ros.fit_resample(Speaker_gender_train_ori_X, Speaker_gender_train_ori_y)

value_counts = y_resampled.value_counts()
plt.figure(figsize=(10, 6))
value_counts.plot(kind='bar')
plt.title('Value Counts of Category')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

Speaker_gender_train = pd.concat([X_resampled,y_resampled],axis=1)

"""#### Correlation Analysis"""

correlation_matrix = Speaker_gender_train.corr()
correlation_matrix

threshold = 0.5

mask = np.abs(correlation_matrix) > threshold
correlated_features = set()

for i in range(len(mask.columns)):
    for j in range(i):
        if mask.iloc[i, j]:
            colname= correlation_matrix.columns[i] if correlation_matrix.iloc[i,-1] <= correlation_matrix.iloc[j,-1] else correlation_matrix.columns[j]
            correlated_features.add(colname)

Speaker_gender_train_dropped =  Speaker_gender_train.drop(columns=correlated_features)
Speaker_gender_valid_dropped = Speaker_gender_valid_ori.drop(columns=correlated_features)

Speaker_gender_train_dropped.shape

"""#### PCA"""

PCA_analysis_df = Speaker_gender_train_dropped.drop(columns=['label_3'])
PCA_analysis_df_valid = Speaker_gender_valid_dropped.drop(columns=['label_3'])

# Standardizing the data (important for PCA)
scaler = StandardScaler()
scaled_data = scaler.fit_transform(PCA_analysis_df)

scaled_valid_data = scaler.transform(PCA_analysis_df_valid)

pca = PCA(n_components=0.95,svd_solver='full')
principal_components = pca.fit_transform(scaled_data)

valid_principal_components = pca.transform(scaled_valid_data)

principal_df = pd.DataFrame(data=principal_components, columns=[f'Principal Component {i}' for i in range(principal_components.shape[1])])
valid_principal_df = pd.DataFrame(data = valid_principal_components,columns=[f'Principal Component {i}' for i in range(valid_principal_components.shape[1])])

principal_df.shape

"""#### Fit a SVM Classifier"""

clf = svm.SVC()
clf.fit(principal_df, Speaker_gender_train_dropped['label_3'])
y_pred = clf.predict(valid_principal_df)
accuracy = accuracy_score(Speaker_gender_valid_dropped['label_3'], y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

"""### Label 04 - Speaker Accent

"""

value_counts = Speaker_accent_train_ori['label_4'].value_counts()
plt.figure(figsize=(10, 6))
value_counts.plot(kind='bar')
plt.title('Value Counts of Category')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

Speaker_accent_train_X = Speaker_accent_train_ori.drop(columns=['label_4'])
Speaker_accent_train_y = Speaker_accent_train_ori['label_4']

"""First handle the class imbalance"""

ros = RandomOverSampler(random_state=44)
X_resampled, y_resampled = ros.fit_resample(Speaker_accent_train_X, Speaker_accent_train_y)
value_counts = y_resampled.value_counts()
plt.figure(figsize=(10, 6))
value_counts.plot(kind='bar')
plt.title('Value Counts of Category')
plt.xlabel('Category')
plt.ylabel('Count')
plt.show()

Speaker_accent_train = pd.concat([X_resampled,y_resampled],axis=1)

correlation_matrix = Speaker_accent_train.corr()
correlation_matrix

threshold = 0.4

mask = np.abs(correlation_matrix) > threshold
correlated_features = set()

for i in range(len(mask.columns)):
    for j in range(i):
        if mask.iloc[i, j]:
            colname= correlation_matrix.columns[i] if correlation_matrix.iloc[i,-1] <= correlation_matrix.iloc[j,-1] else correlation_matrix.columns[j]
            correlated_features.add(colname)

Speaker_accent_train_dropped =  Speaker_accent_train.drop(columns=correlated_features)
Speaker_accent_valid_dropped = Speaker_accent_valid_ori.drop(columns=correlated_features)

Speaker_accent_train_dropped.shape

PCA_analysis_df = Speaker_accent_train_dropped.drop(columns=['label_4'])
PCA_analysis_df_valid = Speaker_accent_valid_dropped.drop(columns=['label_4'])

# Standardizing the data (important for PCA)
scaler = StandardScaler()
scaled_data = scaler.fit_transform(PCA_analysis_df)

scaled_valid_data = scaler.transform(PCA_analysis_df_valid)

pca = PCA(n_components=0.95,svd_solver='full')
principal_components = pca.fit_transform(scaled_data)
valid_principal_components = pca.transform(scaled_valid_data)
principal_df = pd.DataFrame(data=principal_components, columns=[f'Principal Component {i}' for i in range(principal_components.shape[1])])
valid_principal_df = pd.DataFrame(data = valid_principal_components,columns=[f'Principal Component {i}' for i in range(valid_principal_components.shape[1])])

principal_df.shape

clf = svm.SVC()
clf.fit(principal_df, Speaker_accent_train_dropped['label_4'])
y_pred = clf.predict(valid_principal_df)
accuracy = accuracy_score(Speaker_accent_valid_dropped['label_4'], y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

"""# Step 04: Comparison of Results and Dataset generation

## Label 01

For the initial feature set
"""

Speaker_ID_train_ori_X = Speaker_ID_train_ori.drop(columns=['label_1'])
Speaker_ID_train_ori_y = Speaker_ID_train_ori['label_1']

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(Speaker_ID_train_ori_X, Speaker_ID_train_ori_y)

test_ori = pd.read_csv('test_new.csv').drop(columns=['label_1','label_2','label_3','label_4'])
test_ori.head()

initial_label_1_predict = rf_model.predict(test_ori)

test_dropped = test_ori.drop(columns=correlated_features)
PCA_analysis_df_test_scaled = scaler.transform(test_dropped)
test_principal_components = pca.transform(PCA_analysis_df_test_scaled)
test_principal_df = pd.DataFrame(data = test_principal_components,columns=[f'New Feature {i}' for i in range(test_principal_components.shape[1])])

test_principal_df.head()

final_label_1_predict = rf_model.predict(test_principal_df)

data = {
    'Predicted labels before feature engineering': initial_label_1_predict,
    'Predicted labels after feature engineering': final_label_1_predict,
    'No of new features': principal_df.shape[1],
}

label_1_df = pd.DataFrame(data)

label_1_df_final = pd.concat([label_1_df,test_principal_df],axis=1)

label_1_df_final.to_csv('190601D_label_1.csv', index=False)

"""## Label 02"""

Speaker_age_train_ori_cleaned = Speaker_age_train_ori.dropna()

Speaker_age_train_ori_X = Speaker_age_train_ori_cleaned.drop(columns=['label_2']).dropna()
Speaker_age_train_ori_y = Speaker_age_train_ori_cleaned['label_2'].dropna()

clf = svm.SVC()
clf.fit(Speaker_age_train_ori_X, Speaker_age_train_ori_y)

test_ori = pd.read_csv('test_new.csv').drop(columns=['label_1','label_2','label_3','label_4'])
test_ori.head()

initial_label_2_predict = clf.predict(test_ori)

test_dropped = test_ori.drop(columns=correlated_features)
PCA_analysis_df_test_scaled = scaler.transform(test_dropped)
test_principal_components = pca.transform(PCA_analysis_df_test_scaled)
test_principal_df = pd.DataFrame(data = test_principal_components,columns=[f'New Feature {i}' for i in range(test_principal_components.shape[1])])

final_label_2_predict = clf.predict(test_principal_df)

data = {
    'Predicted labels before feature engineering': initial_label_2_predict,
    'Predicted labels after feature engineering': final_label_2_predict,
    'No of new features': principal_df.shape[1],
}

label_2_df = pd.DataFrame(data)

label_2_df_final = pd.concat([label_2_df,test_principal_df],axis=1)

label_2_df_final.to_csv('190601D_label_2.csv', index=False)

"""## Label 03"""

Speaker_gender_train_ori_X = Speaker_gender_train_ori.drop(columns=['label_3'])
Speaker_gender_train_ori_y = Speaker_gender_train_ori['label_3']

clf = svm.SVC()
clf.fit(Speaker_gender_train_ori_X,Speaker_gender_train_ori_y)

test_ori = pd.read_csv('test_new.csv').drop(columns=['label_1','label_2','label_3','label_4'])
test_ori.head()

initial_label_3_predict = clf.predict(test_ori)

test_dropped = test_ori.drop(columns=correlated_features)
PCA_analysis_df_test_scaled = scaler.transform(test_dropped)
test_principal_components = pca.transform(PCA_analysis_df_test_scaled)
test_principal_df = pd.DataFrame(data = test_principal_components,columns=[f'New Feature {i}' for i in range(test_principal_components.shape[1])])

final_label_3_predict = clf.predict(test_principal_df)

data = {
    'Predicted labels before feature engineering': initial_label_3_predict,
    'Predicted labels after feature engineering': final_label_3_predict,
    'No of new features': principal_df.shape[1],
}

label_3_df = pd.DataFrame(data)

label_3_df_final = pd.concat([label_3_df,test_principal_df],axis=1)

label_3_df_final.to_csv('190601D_label_3.csv', index=False)

"""## Label 04"""

Speaker_accent_train_ori_X = Speaker_accent_train_ori.drop(columns=['label_4'])
Speaker_accent_train_ori_y = Speaker_accent_train_ori['label_4']

clf = svm.SVC()
clf.fit(Speaker_accent_train_ori_X,Speaker_accent_train_ori_y)

test_ori = pd.read_csv('test_new.csv').drop(columns=['label_1','label_2','label_3','label_4'])
test_ori.head()

initial_label_4_predict = clf.predict(test_ori)

test_dropped = test_ori.drop(columns=correlated_features)
PCA_analysis_df_test_scaled = scaler.transform(test_dropped)
test_principal_components = pca.transform(PCA_analysis_df_test_scaled)
test_principal_df = pd.DataFrame(data = test_principal_components,columns=[f'New Feature {i}' for i in range(test_principal_components.shape[1])])

final_label_4_predict = clf.predict(test_principal_df)

data = {
    'Predicted labels before feature engineering': initial_label_4_predict,
    'Predicted labels after feature engineering': final_label_4_predict,
    'No of new features': principal_df.shape[1],
}

label_4_df = pd.DataFrame(data)

label_4_df_final = pd.concat([label_4_df,test_principal_df],axis=1)

label_4_df_final.to_csv('190601D_label_4.csv', index=False)

label_4_df_final.head(100)